{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b21d569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Flask in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (1.1.2)\n",
      "Requirement already satisfied: pandas in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (1.4.4)\n",
      "Requirement already satisfied: numpy in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (1.24.4)\n",
      "Requirement already satisfied: scikit-learn in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (0.24.2)\n",
      "Requirement already satisfied: nltk in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from Flask) (2.0.3)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from Flask) (2.11.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from Flask) (2.0.1)\n",
      "Requirement already satisfied: click>=5.1 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from Flask) (8.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from Jinja2>=2.10.1->Flask) (2.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Flask pandas numpy scikit-learn nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "697fcbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72129</th>\n",
       "      <td>72129</td>\n",
       "      <td>Russians steal research on Trump in hack of U....</td>\n",
       "      <td>WASHINGTON (Reuters) - Hackers believed to be ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72130</th>\n",
       "      <td>72130</td>\n",
       "      <td>WATCH: Giuliani Demands That Democrats Apolog...</td>\n",
       "      <td>You know, because in fantasyland Republicans n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72131</th>\n",
       "      <td>72131</td>\n",
       "      <td>Migrants Refuse To Leave Train At Refugee Camp...</td>\n",
       "      <td>Migrants Refuse To Leave Train At Refugee Camp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72132</th>\n",
       "      <td>72132</td>\n",
       "      <td>Trump tussle gives unpopular Mexican leader mu...</td>\n",
       "      <td>MEXICO CITY (Reuters) - Donald Trump’s combati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72133</th>\n",
       "      <td>72133</td>\n",
       "      <td>Goldman Sachs Endorses Hillary Clinton For Pre...</td>\n",
       "      <td>Goldman Sachs Endorses Hillary Clinton For Pre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                              title  \\\n",
       "72129       72129  Russians steal research on Trump in hack of U....   \n",
       "72130       72130   WATCH: Giuliani Demands That Democrats Apolog...   \n",
       "72131       72131  Migrants Refuse To Leave Train At Refugee Camp...   \n",
       "72132       72132  Trump tussle gives unpopular Mexican leader mu...   \n",
       "72133       72133  Goldman Sachs Endorses Hillary Clinton For Pre...   \n",
       "\n",
       "                                                    text  label  \n",
       "72129  WASHINGTON (Reuters) - Hackers believed to be ...      0  \n",
       "72130  You know, because in fantasyland Republicans n...      1  \n",
       "72131  Migrants Refuse To Leave Train At Refugee Camp...      0  \n",
       "72132  MEXICO CITY (Reuters) - Donald Trump’s combati...      0  \n",
       "72133  Goldman Sachs Endorses Hillary Clinton For Pre...      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('WELFake_Dataset.csv')\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb92f3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      0\n",
       "title         558\n",
       "text           39\n",
       "label           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9fd13b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={'Unnamed: 0': 'id'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7226ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd25b3ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819e2696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79e60aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ujjwalsamanta/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ujjwalsamanta/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Download NLTK stopwords data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bc35da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  \\\n",
      "0      No comment is expected from Barack Obama Membe...   \n",
      "1         Did they post their votes for Hillary already?   \n",
      "2       Now, most of the demonstrators gathered last ...   \n",
      "3      A dozen politically active pastors came here f...   \n",
      "4      The RS-28 Sarmat missile, dubbed Satan 2, will...   \n",
      "...                                                  ...   \n",
      "72129  WASHINGTON (Reuters) - Hackers believed to be ...   \n",
      "72130  You know, because in fantasyland Republicans n...   \n",
      "72131  Migrants Refuse To Leave Train At Refugee Camp...   \n",
      "72132  MEXICO CITY (Reuters) - Donald Trump’s combati...   \n",
      "72133  Goldman Sachs Endorses Hillary Clinton For Pre...   \n",
      "\n",
      "                                       preprocessed_text  \n",
      "0      comment expect barack obama member fukyoflag b...  \n",
      "1                              post vote hillari alreadi  \n",
      "2      demonstr gather last night exercis constitut p...  \n",
      "3      dozen polit activ pastor came privat dinner fr...  \n",
      "4      sarmat missil dub satan replac fli mile per se...  \n",
      "...                                                  ...  \n",
      "72129  washington reuter hacker believ work russian g...  \n",
      "72130  know fantasyland republican never question cit...  \n",
      "72131  migrant refus leav train refuge camp hungari t...  \n",
      "72132  mexico citi reuter donald trump comb style buf...  \n",
      "72133  goldman sach endors hillari clinton presid gol...  \n",
      "\n",
      "[72134 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "# If 'text' column contains non-string elements, you can convert them to string\n",
    "data['text'] = data['text'].astype(str)\n",
    "\n",
    "# Function to remove HTML tags\n",
    "def remove_html_tags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "# Function to remove URLs\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = remove_html_tags(text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = remove_urls(text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Remove numbers\n",
    "    tokens = [word for word in tokens if not word.isdigit()]\n",
    "\n",
    "    # Lowercasing\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the 'text' column\n",
    "data['preprocessed_text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Display the result\n",
    "print(data[['text', 'preprocessed_text']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5395c64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f86339e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the preprocessed text\n",
    "data['tokenized_text'] = data['preprocessed_text'].apply(lambda x: x.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71dad9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Training the Word2Vec model on the tokenized text\n",
    "tokenized_text = data['tokenized_text'].tolist()\n",
    "word2vec_model = Word2Vec(tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ffb39fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def document_vector(word2vec_model, doc):\n",
    "    # Remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in word2vec_model.wv.key_to_index]\n",
    "    if not doc:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    return np.mean(word2vec_model.wv[doc], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbaea331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the dataset\n",
    "X = np.array([document_vector(word2vec_model, doc) for doc in tokenized_text])\n",
    "\n",
    "# Assuming your labels are in the 'label' column\n",
    "y = data['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94f84f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a427429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50493\n",
      "21641\n",
      "50493\n",
      "21641\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d958e038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and train the classifier\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8d0c816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.999306871216672\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     10613\n",
      "           1       1.00      1.00      1.00     11028\n",
      "\n",
      "    accuracy                           1.00     21641\n",
      "   macro avg       1.00      1.00      1.00     21641\n",
      "weighted avg       1.00      1.00      1.00     21641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c6c4fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scores: 0.8973522221118081\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90     10613\n",
      "           1       0.89      0.92      0.90     11028\n",
      "\n",
      "    accuracy                           0.90     21641\n",
      "   macro avg       0.90      0.90      0.90     21641\n",
      "weighted avg       0.90      0.90      0.90     21641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('WELFake_Dataset.csv')\n",
    "data['text'] = data['text'].astype(str)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags and URLs\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Tokenize and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the 'text' column\n",
    "data['preprocessed_text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Tokenize the preprocessed text for Word2Vec\n",
    "data['tokenized_text'] = data['preprocessed_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(data['tokenized_text'].tolist(), vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to create document vectors\n",
    "def document_vector(word2vec_model, doc):\n",
    "    doc = [word for word in doc if word in word2vec_model.wv.key_to_index]\n",
    "    if not doc:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    return np.mean(word2vec_model.wv[doc], axis=0)\n",
    "\n",
    "# Vectorize the dataset\n",
    "X = np.array([document_vector(word2vec_model, doc) for doc in data['tokenized_text']])\n",
    "y = data['label'].values\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train RandomForestClassifier with some basic hyperparameters\n",
    "classifier = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier using cross-validation on the training set\n",
    "cv_scores = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "print(f\"CV Scores: {cv_scores.mean()}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea154906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading torch-2.2.2-cp39-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.2-cp39-cp39-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.2-cp39-cp39-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: fsspec in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from torch) (2022.7.1)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/ujjwalsamanta/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch) (1.2.1)\n",
      "Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.2-cp39-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.2-cp39-cp39-macosx_10_12_x86_64.whl (426 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.4/426.4 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp39-cp39-macosx_10_12_x86_64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: typing-extensions, safetensors, fsspec, torch, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Uninstalling typing_extensions-4.3.0:\n",
      "      Successfully uninstalled typing_extensions-4.3.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.7.1\n",
      "    Uninstalling fsspec-2022.7.1:\n",
      "      Successfully uninstalled fsspec-2022.7.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2024.3.1 huggingface-hub-0.22.2 safetensors-0.4.2 tokenizers-0.15.2 torch-2.2.2 transformers-4.39.3 typing-extensions-4.10.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859d1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ff9958",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf11c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e252ccc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6s/hdwy5qm16y7grjqwnqg7blxh0000gn/T/ipykernel_15275/181648228.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Tokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preprocessed_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "texts = data['preprocessed_text'].tolist()\n",
    "labels = data['label'].tolist()\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Padding sequences\n",
    "maxlen = 100  # Assuming maximum sequence length\n",
    "sequences = pad_sequences(sequences, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f1a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(sequences, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7e1762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Embedding\n",
    "word2vec_model = Word2Vec(sentences=texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model:\n",
    "        embedding_matrix[i] = word2vec_model[word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e673e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8821c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the trained model\n",
    "model_path = 'LSTM_fake_news_detection_model.joblib'\n",
    "dump(model, model_path)\n",
    "\n",
    "print(\"Model saved successfully as\", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d79bd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa1c970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "y_pred = model.predict_classes(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
